# input data with columns as characteristics and rows as observations
life<-read.table('/Users/Ren/Desktop/Life.txt', header=TRUE)

#Hierarchical Agglomerative Clustering
# show three graphs at once
par(mfrow=c(1,3))
# single linkage = d_AB = min(d_ij)
plclust(hclust(dist(life),method="single"),labels=row.names(life),ylab="Distance")
title("(a) Single linkage")
# complete linkage = d_AB = max(d_ij)
plclust(hclust(dist(life),method="complete"),labels=row.names(life),ylab="Distance")
title("(b) Complete linkage")
# average linkage = d_AB = d_ij/(n_i*n_j)
plclust(hclust(dist(life),method="average"),labels=row.names(life),ylab="Distance")
title("(c) Average linkage")

# select five clusters
five<-cutree(hclust(dist(life),method="complete"), k=5)
country.clus<-lapply(1:5,function(nc) row.names(life)[five==nc])
country.clus

#set the device off and show one graph at once.
dev.off()

# Ward Hierarchical Clustering
d <- dist(life, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward")
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(fit, k=5, border="red")

#K-means clustering

# Determine number of clusters (The break between the steep slope and a leveling off indicates the number of meaningful clusters)
wss <- (nrow(life)-1)*sum(apply(life,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(life, 
  	 centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")

#K-means clustering
life.kmean<-kmeans(life,3)
life.kmean
lapply(1:3,function(nc) row.names(life)[life.kmean$cluster==nc])
#plot a graph
library(cluster)
 